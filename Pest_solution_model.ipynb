{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DQ7-tFtBqkT4gmLcjCCW2LrlUK_jB7Wk",
      "authorship_tag": "ABX9TyOWDUe418IHq9rxOMTXMX3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeq-shaffy/DL-labsheets/blob/main/Pest_solution_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53d083fb"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8288a58d",
        "outputId": "e2f0a2ca-d9bb-48e0-ce34-9eacf7f354a9"
      },
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"imbikramsaha/paddy-doctor\")\n",
        "print(\"Dataset path:\", path)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'paddy-doctor' dataset.\n",
            "Dataset path: /kaggle/input/paddy-doctor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ee4bb33"
      },
      "source": [
        "dataset_root_path = os.path.join(path, 'paddy-disease-classification')\n",
        "train_csv_path = os.path.join(dataset_root_path, 'train.csv')\n",
        "train_images_path = os.path.join(dataset_root_path, 'train_images')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57ab336d"
      },
      "source": [
        "full_df = pd.read_csv(train_csv_path)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZVDbUHaP-QJ",
        "outputId": "7f8c2b9b-1a51-43fa-d5d0-a308177aa878"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_id', 'label', 'variety', 'age'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(full_df[\"label\"].unique())\n",
        "print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l453HTAfQEQn",
        "outputId": "32c92d3b-e172-4c57-9ff8-71bd159e332b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/kaggle/input/paddy-doctor\"\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files[:5]:  # only first 5 files\n",
        "        print(f\"{subindent}{f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh8HWO4VUSep",
        "outputId": "faf95777-f245-409a-929c-25d43b0061be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paddy-doctor/\n",
            "    paddy-disease-classification/\n",
            "        sample_submission.csv\n",
            "        .jovianrc\n",
            "        train.csv\n",
            "        train_images/\n",
            "            tungro/\n",
            "                109629.jpg\n",
            "                104765.jpg\n",
            "                109706.jpg\n",
            "                100098.jpg\n",
            "                102734.jpg\n",
            "            hispa/\n",
            "                100502.jpg\n",
            "                107167.jpg\n",
            "                106262.jpg\n",
            "                102866.jpg\n",
            "                104880.jpg\n",
            "            downy_mildew/\n",
            "                101119.jpg\n",
            "                105381.jpg\n",
            "                110270.jpg\n",
            "                110143.jpg\n",
            "                105940.jpg\n",
            "            bacterial_leaf_streak/\n",
            "                103494.jpg\n",
            "                106019.jpg\n",
            "                103874.jpg\n",
            "                103325.jpg\n",
            "                109858.jpg\n",
            "            bacterial_leaf_blight/\n",
            "                109940.jpg\n",
            "                105979.jpg\n",
            "                104324.jpg\n",
            "                109428.jpg\n",
            "                106615.jpg\n",
            "            brown_spot/\n",
            "                104821.jpg\n",
            "                100229.jpg\n",
            "                101440.jpg\n",
            "                109636.jpg\n",
            "                109970.jpg\n",
            "            blast/\n",
            "                107934.jpg\n",
            "                108677.jpg\n",
            "                109170.jpg\n",
            "                108044.jpg\n",
            "                107330.jpg\n",
            "            normal/\n",
            "                109776.jpg\n",
            "                101562.jpg\n",
            "                106498.jpg\n",
            "                102410.jpg\n",
            "                104028.jpg\n",
            "            dead_heart/\n",
            "                109841.jpg\n",
            "                102631.jpg\n",
            "                104707.jpg\n",
            "                101276.jpg\n",
            "                107110.jpg\n",
            "            bacterial_panicle_blight/\n",
            "                107921.jpg\n",
            "                109434.jpg\n",
            "                102955.jpg\n",
            "                104569.jpg\n",
            "                103733.jpg\n",
            "        .ipynb_checkpoints/\n",
            "            paddy doctor img modification-checkpoint.ipynb\n",
            "        test_images/\n",
            "            200607.jpg\n",
            "            202733.jpg\n",
            "            200207.jpg\n",
            "            203024.jpg\n",
            "            203056.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f96a8081",
        "outputId": "aa2159a9-c27f-4522-d043-312b38381a2d"
      },
      "source": [
        "train_df, val_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.1,\n",
        "    stratify=full_df['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(len(train_df), len(val_df))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9366 1041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "label_counts = train_df['label'].value_counts()\n",
        "label_order = sorted(train_df['label'].unique())\n",
        "\n",
        "counts = np.array([label_counts[label] for label in label_order], dtype=float)\n",
        "class_weights = 1.0 / counts\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "# Keras expects a dictionary: {class_index: weight}\n",
        "class_index_mapping = {label: idx for idx, label in enumerate(label_order)}\n",
        "class_weights_dict = {class_index_mapping[label]: weight for label, weight in zip(label_order, class_weights)}\n",
        "\n",
        "print(class_weights_dict)\n"
      ],
      "metadata": {
        "id": "AeW_ox4G2_qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d26d2a7-6089-45df-d1fe-7031d928ad31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: np.float64(0.1521780416341574), 1: np.float64(0.19177992966175975), 2: np.float64(0.21646447506376845), 3: np.float64(0.04193653193370962), 4: np.float64(0.07556305984368876), 5: np.float64(0.05053061320826028), 6: np.float64(0.11754253753462696), 7: np.float64(0.045706436198133686), 8: np.float64(0.041302730443527606), 9: np.float64(0.06699564447836756)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "DATA_DIR = \"/kaggle/input/paddy-doctor/paddy-disease-classification/train_images\"\n",
        "EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "tPZ9Ej9UG73-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f260eebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc559a6c-6a7e-4da1-f3f0-4ef6af3c70da"
      },
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    image_size=(224,224),\n",
        "    batch_size=32,\n",
        "    label_mode=\"int\",          # üî• CHANGE THIS\n",
        "    validation_split=0.1,\n",
        "    subset=\"training\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    image_size=(224,224),\n",
        "    batch_size=32,\n",
        "    label_mode=\"int\",          # üî• CHANGE THIS\n",
        "    validation_split=0.1,\n",
        "    subset=\"validation\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "class_names = train_dataset.class_names\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10407 files belonging to 10 classes.\n",
            "Using 9367 files for training.\n",
            "Found 10407 files belonging to 10 classes.\n",
            "Using 1040 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: (preprocess_input(x), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    lambda x, y: (preprocess_input(x), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n"
      ],
      "metadata": {
        "id": "oChpyPKgW6Is"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360aa4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e59950-74b0-42e6-f4d3-d2185b48d2f5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "label_counts = {i: 0 for i in range(len(class_names))}\n",
        "\n",
        "for _, labels in train_dataset.unbatch():\n",
        "    label_counts[int(labels.numpy())] += 1\n",
        "\n",
        "class_weights_dict = {\n",
        "    i: max(label_counts.values()) / count\n",
        "    for i, count in label_counts.items()\n",
        "}\n",
        "\n",
        "print(class_weights_dict)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 3.6492027334851938, 1: 4.711764705882353, 2: 5.322259136212624, 3: 1.0203821656050955, 4: 1.8287671232876712, 5: 1.237065637065637, 6: 2.907441016333938, 7: 1.132155477031802, 8: 1.0, 9: 1.638036809815951}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "num_classes = len(class_names)\n",
        "\n",
        "base_model = EfficientNetB0(\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 1: Train head first\n",
        "# ----------------------------\n",
        "base_model.trainable = False  # freeze backbone\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile for head training\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train head first\n",
        "history_head = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 2: Fine-tune top layers\n",
        "# ----------------------------\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "Oo7XMurL5vtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef46274c-c7ff-48ae-9095-ff83cbe070e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m261/293\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:08\u001b[0m 2s/step - accuracy: 0.3563 - loss: 1.8732"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1edf94fd"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1Ô∏è‚É£ Save the trained Keras model (optional)\n",
        "model.save(\"pest_disease_detection_model.h5\")  # HDF5 backup\n",
        "\n",
        "# 2Ô∏è‚É£ Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Optional optimization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# 3Ô∏è‚É£ Save as TFLite\n",
        "with open(\"pest_disease_detection_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TFLite model saved as pest_disease_detection_model.tflite\")\n"
      ],
      "metadata": {
        "id": "Xp8DhZKHhIij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tntiphan/paddy-rice-disease-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "HZKBUdyuhaVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1Ô∏è‚É£ Load the TFLite model\n",
        "tflite_model_path = \"/content/pest_disease_detection_model.tflite\"\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# 2Ô∏è‚É£ Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "print(f\"Model input shape: {input_shape}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Function to predict a single image\n",
        "def tflite_predict(image):\n",
        "    # Resize and normalize image\n",
        "    img = tf.image.resize(image, (input_shape[1], input_shape[2]))\n",
        "    img = tf.expand_dims(img, axis=0)  # batch dimension\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # normalization\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], img.numpy())\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return np.argmax(output, axis=1)[0]\n",
        "\n",
        "# Example usage:\n",
        "# pred_class = tflite_predict(some_image_tensor)\n",
        "# print(\"Predicted class index:\", pred_class)\n"
      ],
      "metadata": {
        "id": "pLQae_OQh6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/tntiphan/paddy-rice-disease-classification/versions/7\"\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    # Only show first 5 files\n",
        "    for f in files[:5]:\n",
        "        print(f\"{subindent}{f}\")\n"
      ],
      "metadata": {
        "id": "ppv78RYMiGgl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}